#  Swift Semantic Search ğŸ

![Preview](https://github.com/ashvardanian/ashvardanian/blob/master/repositories/SwiftSemanticSearch.jpg?raw=true#center)

This Swift demo app shows you how to build real-time native AI-powered apps for Apple devices using Unum's Swift libraries.
Under the hood, it uses [UForm](https://github.com/unum-cloud/uform) to understand and "embed" multimodal data, like images, multilingual texts, and ğŸ”œ videos.
Once the embeddings are computed, it uses [USearch](https://github.com/unum-cloud/usearch) to provide real-time search over the semantic space.
That same engine also enables geo-spatial search over the coordinates of the images, and has been to shown to easily scale even to 100M+ entries on an iPhone ğŸ

<table>
  <tr>
    <td>
      <img src="https://github.com/ashvardanian/ashvardanian/blob/master/demos/SwiftSemanticSearch-Dog.gif?raw=true" alt="SwiftSemanticSearch demo Dog">
    </td>
    <td>
      <img src="https://github.com/ashvardanian/ashvardanian/blob/master/demos/SwiftSemanticSearch-Flowers.gif?raw=true" alt="SwiftSemanticSearch demo with Flowers">
    </td>
  </tr>
</table>
